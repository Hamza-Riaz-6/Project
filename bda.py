# -*- coding: utf-8 -*-
"""bda.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d8pVDAgynL5NKxZeuhq51hzksg1a1BpG
"""

# Necessary Libraries
import os
import numpy as np
import pandas as pd
import librosa
import librosa.display
import IPython.display as ipd
import matplotlib.pyplot as plt
from tqdm import tqdm

pip install tqdm

# Path to the directory where MP3s are stored
AUDIO_DIR = 'E:/BIG DATASET/fma_large'

# Helper function to get the full path of an audio file
def get_audio_path(audio_dir, track_id):
    tid_str = '{:06d}'.format(track_id)
    return os.path.join(audio_dir, tid_str[:3], tid_str + '.mp3')

# Load metadata
tracks = pd.read_csv('E:/BIG DATASET/fma_metadata/tracks.csv')

def extract_features(file_path):
    # Load the audio file
    y, sr = librosa.load(file_path, sr=None, mono=True)

    # Extracting various features
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)
    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)
    chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)
    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)

    # You can add more features here

    # Returning the mean of the features for simplicity
    return {
        'mfcc': np.mean(mfcc, axis=1),
        'spectral_centroid': np.mean(spectral_centroid),
        'chroma_cens': np.mean(chroma_cens, axis=1),
        'spectral_contrast': np.mean(spectral_contrast, axis=1)
    }
track_id = 2
audio_path = get_audio_path(AUDIO_DIR, track_id)
features = extract_features(audio_path)

print("Extracted Features:", features)

# Path to the directory where MP3s are stored
AUDIO_DIR = 'E:/BIG DATASET/fma_large'

# Helper function to get the full path of an audio file
def get_audio_path(audio_dir, track_id):
    tid_str = '{:06d}'.format(track_id)
    return os.path.join(audio_dir, tid_str[:3], tid_str + '.mp3')

# Load metadata
tracks = pd.read_csv('E:/BIG DATASET/fma_metadata/tracks.csv', index_col=0)

def extract_features(file_path):
    try:
        y, sr = librosa.load(file_path, sr=None, mono=True)
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)
        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)
        chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)
        spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)
        return {
            'mfcc': np.mean(mfcc, axis=1),
            'spectral_centroid': np.mean(spectral_centroid),
            'chroma_cens': np.mean(chroma_cens, axis=1),
            'spectral_contrast': np.mean(spectral_contrast, axis=1)
        }
    except Exception as e:
        print(f"Error processing {file_path}: {e}")
        return None

# List to hold feature dictionaries before converting to DataFrame
features_list = []

# Initialize a counter to manage saving in chunks
counter = 0
save_path = 'E:/BIG DATASET/extracted_features.csv'

for track_id in tqdm(range(2, 154358)):  # Adjust the range as needed
    audio_path = get_audio_path(AUDIO_DIR, track_id)
    features = extract_features(audio_path)
    if features:
        features['track_id'] = track_id
        features_list.append(features)

    # Save every 1000 entries
    if len(features_list) >= 1000:
        df = pd.DataFrame(features_list)
        if counter == 0:
            df.to_csv(save_path, index=False)  # Overwrite existing file
        else:
            df.to_csv(save_path, mode='a', header=False, index=False)  # Append to the file
        features_list = []  # Reset the list
        counter += 1

# Save any remaining data
if features_list:
    df = pd.DataFrame(features_list)
    if counter == 0:
        df.to_csv(save_path, index=False)
    else:
        df.to_csv(save_path, mode='a', header=False, index=False)

print("Feature extraction complete. Data saved to 'extracted_features.csv'.")

tracks = pd.read_csv(r'C:\Users\bilal\Downloads\Compressed\fma_metadata\fma_metadata/raw_tracks.csv', encoding='utf-8')
genres = pd.read_csv(r'C:\Users\bilal\Downloads\Compressed\fma_metadata\fma_metadata/raw_genres.csv', encoding='utf-8')
artists = pd.read_csv(r'C:\Users\bilal\Downloads\Compressed\fma_metadata\fma_metadata/raw_artists.csv', encoding='utf-8')
echonest = pd.read_csv(r'C:\Users\bilal\Downloads\Compressed\fma_metadata\fma_metadata/raw_echonest.csv', encoding='utf-8')
audio_feature = pd.read_csv(r'C:\Users\bilal\Downloads/extracted_features.csv', encoding='utf-8')

tracks

tracks.isnull().sum()

# Step 1: Calculate the percentage of null values in each column
null_percentages = tracks.isnull().mean() * 100

# Step 2: Remove columns with null percentages equal to or below 5%
columns_to_drop = null_percentages[null_percentages <= 5].index
tracks_cleaned = tracks.drop(columns=columns_to_drop)

# Step 3: Fill null values in remaining columns
for column in tracks_cleaned.columns:
    if tracks_cleaned[column].isnull().any():
        if tracks_cleaned[column].dtype == 'object':
            tracks_cleaned[column].fillna('Unknown', inplace=True)  # Fill with 'Unknown' for object dtype
        else:
            tracks_cleaned[column].fillna(tracks_cleaned[column].median(), inplace=True)  # Fill with median for numeric dtype

# Display the cleaned DataFrame

# Define the columns relevant for music recommendation
relevant_columns = ['track_id', 'album_id', 'artist_id', 'track_title', 'track_duration', 'track_genres', 'track_listens', 'track_interest']

# Filter the DataFrame to keep only relevant columns
tracks_relevant = tracks[relevant_columns]

# Display the DataFrame with relevant columns
tracks_relevant

tracks_relevant.isnull().sum()
tracks_relevant = tracks_relevant.dropna()
tracks_relevant

artists

# Define the columns relevant for audio features
audio_features_columns = ['mfcc', 'spectral_centroid', 'chroma_cens', 'spectral_contrast', 'track_id']

# Filter the 'audio_features' DataFrame to keep only relevant columns
audio_features_relevant = audio_feature[audio_features_columns]

# Merge 'tracks_relevant_cleaned' DataFrame with 'audio_features_relevant' DataFrame on 'track_id'
merged_data = pd.merge(tracks_relevant, audio_features_relevant, on='track_id', how='inner')
merged_data

file_path = "D:\merged_data.csv"

merged_data=pd.read_csv(file_path)

merged_data

import pymongo

# MongoDB connection string
mongo_uri = "mongodb://localhost:27017"

# Connect to MongoDB
client = pymongo.MongoClient(mongo_uri)
db = client["bdaproject"]
collection = db["feature_data"]

# Convert DataFrame to dictionary (records)
records = merged_data.to_dict(orient="records")

# Insert data into MongoDB
collection.insert_many(records)

# Retrieve data from MongoDB and convert to DataFrame
cursor = collection.find({})
df_retrieved = pd.DataFrame(list(cursor))

# Drop the _id column added by MongoDB
df_retrieved.drop("_id", axis=1, inplace=True)

import pymongo

# MongoDB connection string
mongo_uri = "mongodb://localhost:27017"

# Connect to MongoDB
client = pymongo.MongoClient(mongo_uri)
db = client["bdaproject"]
collection = db["feature_data"]
# Retrieve data from MongoDB and convert to DataFrame
cursor = collection.find({})
df_retrieved = pd.DataFrame(list(cursor))

# Drop the _id column added by MongoDB
df_retrieved.drop("_id", axis=1, inplace=True)
df_retrieved

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler
import numpy as np
import pandas as pd

def batch_cosine_similarity(X, batch_size=1000):
    """Compute the cosine similarity in batches."""
    # Placeholder for the full similarity matrix
    sim_matrix = np.zeros((X.shape[0], X.shape[0]))

    for start in range(0, X.shape[0], batch_size):
        end = start + batch_size
        batch_sim = cosine_similarity(X[start:end], X)
        sim_matrix[start:end] = batch_sim

    return sim_matrix

def predict_ratings_batch(item_similarity, data_scaled, batch_size=1000):
    """Predict ratings using item similarity in batches."""
    predicted_ratings = np.zeros(data_scaled.shape)

    for start in range(0, data_scaled.shape[0], batch_size):
        end = start + batch_size
        # Perform dot product
        weighted_sum = item_similarity[start:end].dot(data_scaled)

        # Calculate sum of absolute similarities for normalization
        sum_abs_similarities = np.abs(item_similarity[start:end]).sum(axis=1)
        sum_abs_similarities[sum_abs_similarities == 0] = 1  # Avoid division by zero

        # Normalize the weighted sums
        predicted_ratings[start:end] = weighted_sum / sum_abs_similarities[:, np.newaxis]

    return predicted_ratings

# Load your data
merged_data = df_retrieved
numeric_data = merged_data.drop(['track_title', 'track_genres'], axis=1)
numeric_data = numeric_data.apply(pd.to_numeric, errors='coerce').fillna(0)

# Standardize the numeric data
scaler = StandardScaler()
numeric_data_scaled = scaler.fit_transform(numeric_data)

# Compute similarity in batches
item_similarity = batch_cosine_similarity(numeric_data_scaled, batch_size=1000)

# Calculate predicted ratings in batches
predicted_ratings = predict_ratings_batch(item_similarity, numeric_data_scaled, batch_size=1000)

# Flatten the predicted ratings for evaluation
predicted_ratings_flat = predicted_ratings.flatten()

# Calculate RMSE for evaluation
true_ratings_flat = numeric_data_scaled.flatten()  # Example: flattening actual ratings
rmse = np.sqrt(np.mean((true_ratings_flat - predicted_ratings_flat) ** 2))
print("Root Mean Squared Error (RMSE) =", rmse)

from flask import Flask, render_template, request, redirect, url_for, session
from flask_sqlalchemy import SQLAlchemy
from sqlalchemy.exc import IntegrityError
from sqlalchemy.orm.exc import NoResultFound
import bcrypt

app = Flask(__name__)
app.secret_key = b'_5#y2L"F4Q8z\n\xec]/'

# Configure SQLAlchemy
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///users.db'
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False
db = SQLAlchemy(app)

# Define User model
class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(50), unique=True, nullable=False)
    password = db.Column(db.String(60), nullable=False)

# Create database tables within application context
with app.app_context():
    db.create_all()

@app.route('/')
def index():
    if 'username' in session:
        return render_template('index.html', username=session['username'])
    return redirect(url_for('login'))

@app.route('/signup', methods=['GET', 'POST'])
def signup():
    if request.method == 'POST':
        existing_user = User.query.filter_by(username=request.form['username']).first()
        if existing_user:
            return 'That username already exists!'
        hash_pass = bcrypt.hashpw(request.form['password'].encode('utf-8'), bcrypt.gensalt())
        new_user = User(username=request.form['username'], password=hash_pass.decode('utf-8'))
        db.session.add(new_user)
        try:
            db.session.commit()
        except IntegrityError:
            db.session.rollback()
            return 'An error occurred while creating the user.'
        session['username'] = request.form['username']
        return redirect(url_for('index'))
    return render_template('signup.html')

@app.route('/login', methods=['GET', 'POST'])
def login():
    if request.method == 'POST':
        try:
            user = User.query.filter_by(username=request.form['username']).one()
            if bcrypt.checkpw(request.form['password'].encode('utf-8'), user.password.encode('utf-8')):
                session['username'] = request.form['username']
                return redirect(url_for('index'))
            else:
                return 'Invalid username/password combination'
        except NoResultFound:
            return 'User does not exist'
    return render_template('login.html')

@app.route('/logout')
def logout():
    session.pop('username', None)
    return redirect(url_for('index'))

if __name__ == '__main__':
    app.run(debug=True)





